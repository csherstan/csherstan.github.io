---
title: Successor Representation Literature
date: '2018-07-27T09:59:36+09:00'
tags:
- successor representation
- successor features
- reinforcement learning
- planning
- model-based
- transfer learning
- hippocampus
---
<p>This is a list of literature related to the successor representation. It is not exhaustive and I have not read it all. Right now it is just a list; if I have time I’ll add summaries for those papers I’ve read.</p><p><b>Learning the SR</b></p><ul><li>Dayan, P. (1993). Improving Generalization for Temporal Difference Learning: The Successor Representation. <i>Neural Computation</i>, 5(4), 613–624.<br/></li><li>Gehring, C. A. (2015). Approximate Linear Successor Representation. In <i>Reinforcement Learning Decision Making</i>. Retrieved from <a href="http://people.csail.mit.edu/gehring/publications/clement-gehring-rldm-2015.pdf">http://people.csail.mit.edu/gehring/publications/clement-gehring-rldm-2015.pdf</a><br/></li><li>White, L. M. (1996). <i>Temporal Difference Learning: Eligibility Traces and the Successor Representation for Actions</i>. University of Toronto.<br/></li><li>Pitis, S. (2018). Source Traces for Temporal Difference Learning. In <i>AAAI Conference on Artificial Intelligence</i>. New Orleans, Louisiana, USA.<br/></li></ul><p><b>Transfer</b></p><ul><li>Barreto, A., Dabney, W., Munos, R., Hunt, J., Schaul, T., Silver, D., &amp; van Hasselt, H. (2017). Transfer in Reinforcement Learning with Successor Features and Generalised Policy Improvement. In <i>Lifelong Learning: A Reinforcement Learning Approach Workshop @ICML</i>. Sydney, Australia. <br/></li><li>Barreto, A., Munos, R., Schaul, T., &amp; Silver, D. (2016). Successor Features for Transfer in Reinforcement Learning. arXiv: 1606.05312.<br/></li><li>Barreto, A., Borsa, D., Quan, J., Schaul, T., Silver, D., Hessel, M., … Munos, R. (2018). Transfer in Deep Reinforcement Learning Using Successor Features and Generalised Policy Improvement. <i>International Conference on Machine Learning (ICML).</i><br/></li><li>Zhang, J., Springenberg, J. T., Boedecker, J., &amp; Burgard, W. (2017). Deep Reinforcement Learning with Successor Features for Navigation across Similar Environments. In <i>The International Conference on Intelligent Robots and Systems (IROS)</i> (pp. 2371–2378). Vancouver, Canada.<br/></li><li>Zhu, Y., Gordon, D., Kolve, E., Fox, D., Fei-Fei, L., Gupta, A., … Farhadi, A. (2017). Visual Semantic Planning using Deep Successor Representations. <i>International Conference on Computer Vision</i>, <i>2</i>(4), 7.<br/></li><li>Sherstan, C., Machado, M. C., &amp; Pilarski, P. M. (2018). Accelerating Learning in Constructive Predictive Frameworks with the Successor Representation. <i>IROS. </i>Madrid, Spain.<br/></li><li>Kulkarni, T. D., Saeedi, A., Gautam, S., &amp; Gershman, S. J. (2016). Deep Successor Reinforcement Learning. arXiv: 1606.02396.<br/></li><li>Lehnert, L., Tellex, S., &amp; Littman, M. L. (2017). Advantages and Limitations of using Successor Features for Transfer in Reinforcement Learning. arXiv: 1708.00102<br/></li><li>Ma, C., Wen, J., &amp; Bengio, Y. (2018). Universal Successor Representations for Transfer Reinforcement Learning. arXiv: 1804.03758.<br/></li></ul><p><b>Exploration</b></p><ul><li>Machado, M. C., Rosenbaum, C., Guo, X., Liu, M., Tesauro, G., &amp; Campbell, M. (2018). Eigenoption Discovery Through The Deep Successor Representation. In <i>International Conference on Learning Representations</i>. Vancouver, Canada.<br/></li></ul><p><b>Neuroscience</b></p><ul><li>Stachenfeld, K. L., Botvinick, M. M., &amp; Gershman, S. J. (2017). The Hippocampus as a Predictive Map. <i>Nature Neuroscience</i>, <i>20</i>, 1643–1653. <br/></li><li>Stachenfeld, K. L., Botvinick, M. M., &amp; Gershman, S. J. (2014). Design Principles of the Hippocampal Cognitive Map. <i>Advances in Neural Information Processing Systems</i>, 1–9.<br/></li><li>Russek, E. M., Momennejad, I., Botvinick, M. M., Gershman, S. J., &amp; Daw, N. D. (2017). Predictive Representations Can Link Model-based Reinforcement Learning to Model - free Mechanisms. <i>PLoS Computational Biology</i>, <i>13</i>(9), 1–42.<br/></li><li>Gershman, S. J., Moore, C. D., Todd, M. T., Norman, K. A., &amp; Sederberg, P. B. (2012). The Successor Representation and Temporal Context. <i>Neural Computation</i>, <i>24</i>(6), 1553–1568. <br/></li><li>Banino, A., Barry, C., Uria, B., Blundell, C., Lillicrap, T., Mirowski, P., … Kumaran, D. (2018). Vector-based Navigation using Grid-like Representations in Artificial Agents. <i>Nature</i>, <i>26</i>.<br/></li><li>Ducarouge, A., &amp; Sigaud, O. (2017). The Successor Representation as a Model of Behavioural Flexibility. ?<br/></li><li>Foster, D. J., Morris, R. G. M., &amp; Dayan, P. (2000). A Model of Hippocampally Dependent Navigation, Using the Temporal Difference Learning Rule. <i>Hippocampus</i>, <i>10</i>(1), 1–16.<br/></li><li>Gershman, S. J. (2017). Predicting the past, remembering the future. <i>Current Opinion in Behavioral Sciences</i>, <i>17</i>, 7–13.<br/></li><li>Momennejad, I., Russek, E. M., Cheong, J. H., Botvinick, M. M., Daw, N. D., &amp; Gershman, S. J. (2017). The Successor Representation in Human Reinforcement Learning. <i>Nature Human Behaviour</i>, <i>1</i>(9), 680–692.<br/></li></ul><p><b>Other</b></p><p>There is also a handful of literature that does not necessarily directly make the connection to the SR but uses it nonetheless.</p><ul><li>Yao, H., Szepesvári, C., Sutton, R., Modayil, J., &amp; Bhatnagar, S. (2014). Universal Option Models. <i>Proceedings of the 28th Annual Conference on Neural Information Processing Systems (NIPS)</i>, 1–9.<br/></li></ul><p>There are a number of papers in the field of imitation learning that <i>may</i> be related:</p><ul><li>Apprenticeship Learning Using Linear Programming - Syed, Bowling &amp; Schapire, ICML &lsquo;08 <br/></li><li>Abbeel, P., Ng. A. Y. (2004). Apprenticeship Learning Via Inverse Reinforcement Learning. ICML<br/></li><li>Syed, U. &amp; Schapire, R. A. (2007). Game-Theoretic Approach to Apprenticeship Learning. NIPS<br/></li></ul>
